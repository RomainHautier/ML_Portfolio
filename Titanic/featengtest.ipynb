{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset given is the training set.\n",
      "             Survived  Pclass  Sex        Age     Fare  Cabin CabinLetter\n",
      "PassengerId                                                              \n",
      "1                   0       3    0  22.000000   7.2500    NaN         NaN\n",
      "2                   1       1    1  38.000000  71.2833    0.0           C\n",
      "3                   1       3    1  26.000000   7.9250    NaN         NaN\n",
      "4                   1       1    1  35.000000  53.1000    0.0           C\n",
      "5                   0       3    0  35.000000   8.0500    NaN         NaN\n",
      "...               ...     ...  ...        ...      ...    ...         ...\n",
      "887                 0       2    0  27.000000  13.0000    NaN         NaN\n",
      "888                 1       1    1  19.000000  30.0000    0.0           B\n",
      "889                 0       3    1  29.699118  23.4500    NaN         NaN\n",
      "890                 1       1    0  26.000000  30.0000    0.0           C\n",
      "891                 0       3    0  32.000000   7.7500    NaN         NaN\n",
      "\n",
      "[891 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import FeatEngFun\n",
    "importlib.reload(FeatEngFun)\n",
    "from FeatEngFun import titanic_feature_eng\n",
    "from FeatEngFun import cabin_imputing_fun\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "DATASET = 'train.csv'\n",
    "\n",
    "data = pd.read_csv(DATASET, index_col = 0)\n",
    "\n",
    "data.head()\n",
    "\n",
    "if DATASET == 'test.csv':\n",
    "    if not os.path.exists('scaler.pkl') and not os.path.exists('updated_models.pkl') and not os.path.exists('enable_grid_search.pkl'):\n",
    "        user_input = input(\"One of the files required to perform the operations on the test set could not be found. Would you like to run the program using the training set to solve the errors? (Y/N): \").strip().lower()\n",
    "        if user_input == 'y':\n",
    "            DATASET = 'train.csv'\n",
    "            data = pd.read_csv(DATASET, index_col=0)\n",
    "            [dataframe, cabins] = titanic_feature_eng(data, DATASET, drop_all=True, cabin_pca=True, cabin_group_encoding=True)\n",
    "            print(dataframe)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"The file 'FEng_stats_imputing' could not be found. Please run the program with the training set first.\")\n",
    "\n",
    "[dataframe, cabins] = titanic_feature_eng(data, DATASET, drop_all=True, cabin_pca=False, cabin_group_encoding=True)\n",
    "print(dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average fare price per cluster is:\n",
      "            mean  median\n",
      "Cabin                   \n",
      "0.0    97.835193    79.2\n",
      "1.0    44.515765    35.5\n",
      "0.0    0.593137\n",
      "1.0    0.406863\n",
      "Name: Cabin, dtype: float64\n",
      "Cabin                   0.0      1.0\n",
      "Fare_Interval                       \n",
      "(-0.001, 170.776]   0.55615  0.44385\n",
      "(170.776, 341.553]  1.00000  0.00000\n",
      "(341.553, 512.329]  1.00000  0.00000\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "sns.boxenplot(x = 'Cabin', y = 'Fare', data = dataframe)\n",
    "plt.show()\n",
    "\n",
    "print('The average fare price per cluster is:')\n",
    "print(dataframe.groupby('Cabin')['Fare'].agg(['mean', 'median']))\n",
    "print(dataframe['Cabin'].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "# Assuming your data has 'Fare' and 'cluster' columns\n",
    "\n",
    "# Step 1: Define the number of intervals and calculate fare ranges\n",
    "num_intervals = 3\n",
    "fare_min, fare_max = dataframe['Fare'].min(), dataframe['Fare'].max()\n",
    "fare_bins = np.linspace(fare_min, fare_max, num_intervals + 1)  # Create interval edges\n",
    "\n",
    "# Step 2: Create a new column 'Fare_Interval' to classify fares into intervals\n",
    "dataframe['Fare_Interval'] = pd.cut(dataframe['Fare'], bins=fare_bins, include_lowest=True)\n",
    "\n",
    "# Step 3: Calculate proportions of passengers in clusters 0 and 1 by fare interval\n",
    "\"\"\" fare_cluster_proportions = pd.crosstab(\n",
    "    index=dataframe['Fare_Interval'],      # Fare intervals\n",
    "    columns=dataframe['Cabin'],          # Clusters 0 and 1\n",
    "    normalize='index'                 # Proportion by each interval\n",
    ")[[0, 1]]  # Only select clusters 0 and 1 \"\"\"\n",
    "\n",
    "fare_cluster_proportions = pd.crosstab(\n",
    "    index=dataframe['Fare_Interval'],      # Fare intervals\n",
    "    columns=dataframe['Cabin'],          # Clusters 0 and 1\n",
    "    normalize='index'                 # Proportion by each interval\n",
    ") # Only select clusters 0 and 1\n",
    "\n",
    "\n",
    "# Step 4: Display the result\n",
    "print(fare_cluster_proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average prices for cabins depending on the cabin floor are the following: \n",
      " Cabin\n",
      "0.0    97.835193\n",
      "1.0    44.515765\n",
      "Name: Fare, dtype: float64\n",
      "Survived           0\n",
      "Pclass             0\n",
      "Sex                0\n",
      "Age                0\n",
      "Fare               0\n",
      "Cabin              0\n",
      "CabinLetter      687\n",
      "Fare_Interval      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>CabinLetter</th>\n",
       "      <th>Fare_Interval</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(-0.001, 170.776]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C</td>\n",
       "      <td>(-0.001, 170.776]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(-0.001, 170.776]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C</td>\n",
       "      <td>(-0.001, 170.776]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(-0.001, 170.776]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass  Sex   Age     Fare  Cabin CabinLetter  \\\n",
       "PassengerId                                                            \n",
       "1                   0       3    0  22.0   7.2500    1.0         NaN   \n",
       "2                   1       1    1  38.0  71.2833    0.0           C   \n",
       "3                   1       3    1  26.0   7.9250    1.0         NaN   \n",
       "4                   1       1    1  35.0  53.1000    0.0           C   \n",
       "5                   0       3    0  35.0   8.0500    1.0         NaN   \n",
       "\n",
       "                 Fare_Interval  \n",
       "PassengerId                     \n",
       "1            (-0.001, 170.776]  \n",
       "2            (-0.001, 170.776]  \n",
       "3            (-0.001, 170.776]  \n",
       "4            (-0.001, 170.776]  \n",
       "5            (-0.001, 170.776]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Imputing the Cabins based on the cluster/cabin average fare price.\n",
    "\n",
    "dataframe = cabin_imputing_fun(dataframe)\n",
    "\n",
    "print(dataframe.isna().sum())# Splitting the datasets\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n",
      "\n",
      "\n",
      "The best version of the Logistic Regression model is :\n",
      "\n",
      " LogisticRegression(C=2.3000000000000003, class_weight='balanced',\n",
      "                   max_iter=10000, random_state=42, solver='liblinear') \n",
      "\n",
      "\n",
      "Fitting 2 folds for each of 31 candidates, totalling 62 fits\n",
      "\n",
      "\n",
      "The best version of the Random Forest model is :\n",
      "\n",
      " RandomForestClassifier(class_weight='balanced', criterion='entropy',\n",
      "                       max_depth=5, n_estimators=117, random_state=42) \n",
      "\n",
      "\n",
      "Fitting 2 folds for each of 40 candidates, totalling 80 fits\n",
      "\n",
      "\n",
      "The best version of the Support Vector Machine model is :\n",
      "\n",
      " SVC(class_weight='balanced', probability=True, random_state=42) \n",
      "\n",
      "\n",
      "Fitting 2 folds for each of 30 candidates, totalling 60 fits\n",
      "\n",
      "\n",
      "The best version of the Support Vector Machine 2 model is :\n",
      "\n",
      " SVC(C=0.5, class_weight='balanced', degree=1, kernel='poly', probability=True,\n",
      "    random_state=42) \n",
      "\n",
      "\n",
      "Fitting 2 folds for each of 420 candidates, totalling 840 fits\n",
      "\n",
      "\n",
      "The best version of the XGBoost model is :\n",
      "\n",
      " XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eta=0.4, eval_metric=None,\n",
      "              feature_types=None, gamma=0.01, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=1,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=190,\n",
      "              n_jobs=None, num_parallel_tree=None, ...) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import GridSearch\n",
    "importlib.reload(GridSearch)\n",
    "from GridSearch import gridsearch_fun\n",
    "\n",
    "standardized_columns = ['Age', 'Fare']\n",
    "\n",
    "if DATASET == 'train.csv':\n",
    "    X = dataframe.drop(['Survived', 'CabinLetter', 'Fare_Interval'], axis = 1)\n",
    "    y = dataframe['Survived']\n",
    "\n",
    "    # Dropping the columns uncessary to the models\n",
    "\n",
    "    #dataframe.drop(['CabinLetter', 'Fare_Interval'], axis = 1)\n",
    "\n",
    "    # The dataframe is now clean, ready to be split and used to\n",
    "\n",
    "    # Segmentation of the sets \"X\" et \"y\" into test (20%) and training sets (80%). \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    scaler.fit(X_train[standardized_columns])\n",
    "    # Saving the standard scaler to a file to be reused on the test set\n",
    "    with open('scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    X_train[standardized_columns] = scaler.transform(X_train[standardized_columns])\n",
    "    X_test[standardized_columns] = scaler.transform(X_test[standardized_columns])\n",
    "\n",
    "\n",
    "    ### Performing a gridsearch ###\n",
    "\n",
    "    enable_grid_search = gridsearch_fun(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "\n",
    "elif DATASET == 'test.csv':\n",
    "\n",
    "    # In case the test set is selected: \n",
    "\n",
    "    # 1. The potential models to use on the test set have to be saved\n",
    "    # in the updated_models file, that is after running the model grid-\n",
    "    # search.\n",
    "\n",
    "    # Check if the file exists in the current directory\n",
    "    file_path = 'updated_models.pkl'\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"The file '{file_path}' exists.\")\n",
    "    else:\n",
    "        print(f\"The file '{file_path}' does not exist.\")\n",
    "        raise FileNotFoundError(\"The file 'updated_models' could not be found.\")\n",
    "    \n",
    "    # 2. Dropping the Fare Cluster and the Cabin Letter columns.\n",
    "\n",
    "    X = dataframe.drop(['CabinLetter', 'Fare_Interval'], axis = 1)\n",
    "\n",
    "    # 3. Verifying that no missing values are present in the test set. \n",
    "    # If there are any either choose to impute them or drop the corres-\n",
    "    # ponding rows.\n",
    "\n",
    "    nan_values = X.isna().sum()\n",
    "    print(nan_values)\n",
    "    nan_columns = nan_values[nan_values > 0].index.tolist()\n",
    "    if len(nan_columns) != 0:\n",
    "        print(f\"Columns with NaN values: {nan_columns}\")\n",
    "        user_input = input(\"Would you like to impute the missing values? (Y/N): \").strip().lower()\n",
    "        if user_input == 'n':\n",
    "            X = X.dropna(axis=0)\n",
    "            print(\"The rows with missing values have been dropped.\")\n",
    "        else:\n",
    "            impute_input = input(\"What imputing technique would you like to apply (1: mean, 2: median, 3: mode)? (Y/N): \").strip().lower()\n",
    "            impute_techniques = {1: 'mean', 2: 'median', 3: 'mode'}\n",
    "\n",
    "            if int(impute_input) in impute_techniques.keys():\n",
    "                technique = impute_techniques[int(impute_input)]\n",
    "\n",
    "                if technique == 'mean':\n",
    "                    X[nan_columns] = X[nan_columns].fillna(X[nan_columns].mean())\n",
    "\n",
    "                elif technique == 'median':\n",
    "                    X[nan_columns] = X[nan_columns].fillna(X[nan_columns].median())\n",
    "\n",
    "                elif technique == 'mode':\n",
    "                    X[nan_columns] = X[nan_columns].fillna(X[nan_columns].mode().iloc[0])\n",
    "                print(f\"The missing values have been imputed using {technique}.\")\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Invalid imputing technique selected.\")\n",
    "\n",
    "    # 4. Opening the save models and scaler from the training set and\n",
    "    # scaling the columns the columns that require it. \n",
    "\n",
    "    with open('updated_models', 'rb') as f:\n",
    "        updated_models = pickle.load(f)\n",
    "\n",
    "    with open('scaler.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    with open('enable_grid_search', 'rb') as f:\n",
    "        enable_grid_search = pickle.load(f)\n",
    "\n",
    "  \n",
    "    X[standardized_columns]  = scaler.transform(X[standardized_columns])\n",
    "\n",
    "\n",
    "    # 5. The models have to be used to perform predictions on the test \n",
    "    # set.\n",
    "\n",
    "    # Setting up the directory in which to save the prediction files if\n",
    "    # it does not yet exist.\n",
    "\n",
    "    if not os.path.exists('Predictions'):\n",
    "        os.mkdir('Predictions')\n",
    "\n",
    "    predictions_directory = 'Predictions'\n",
    "    print(X.head())\n",
    "\n",
    "    for model_name in enable_grid_search:\n",
    "        opti_predictions = updated_models[model_name].predict(X)\n",
    "        print(len(opti_predictions))\n",
    "\n",
    "        prediction_filename = f'submission_{model_name}_predictions.csv'\n",
    "\n",
    "        prediction_dataframe = pd.DataFrame({'PassengerId': X.index, 'Survived': opti_predictions})\n",
    "        prediction_dataframe.to_csv(os.path.join(predictions_directory,prediction_filename), index = False, header = True)\n",
    "    \n",
    "    print(f\"The predictions have been saved to the {predictions_directory} directory.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
